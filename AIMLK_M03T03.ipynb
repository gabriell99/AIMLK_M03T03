{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84aeec10-6369-4452-9f78-f39ea45ff4e7",
   "metadata": {},
   "source": [
    "# AIML- INCAF-1\n",
    "## Módulo 3: Diseñar e implementar modelos\n",
    "## Actividad 3. Despliegue de un modelo de Machine Learning en un entorno de producción\n",
    "#### Elaborado por Gabriel Guzmán\n",
    "\n",
    "Se ha preparado un modelo para el despliegue en un entorno de producción, asegurándose de incluir los pasos de limpieza y transformación de un conjunto de datos financieros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec123c2-fe58-4d6e-9f6f-a9daa3c2a0f2",
   "metadata": {},
   "source": [
    "### 1. Carga Información, depuración y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff46f92d-9184-4ca9-91fe-7b1c7d84366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110023 entries, 0 to 110022\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   id               110023 non-null  int64  \n",
      " 1   CustomerId       110023 non-null  int64  \n",
      " 2   Surname          110023 non-null  object \n",
      " 3   CreditScore      110023 non-null  int64  \n",
      " 4   Geography        110023 non-null  object \n",
      " 5   Gender           110023 non-null  object \n",
      " 6   Age              110023 non-null  float64\n",
      " 7   Tenure           110023 non-null  int64  \n",
      " 8   Balance          110023 non-null  float64\n",
      " 9   NumOfProducts    110023 non-null  int64  \n",
      " 10  HasCrCard        110023 non-null  float64\n",
      " 11  IsActiveMember   110023 non-null  float64\n",
      " 12  EstimatedSalary  110023 non-null  float64\n",
      "dtypes: float64(5), int64(5), object(3)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#Importamos librerias\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_curve, auc,precision_recall_curve, average_precision_score\n",
    "\n",
    "# Configuración básica de log\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='app_AIMLK_M03T03.log',  # Guardar logs en un archivo\n",
    "    filemode='w'  # Sobreescribe el archivo en cada ejecución\n",
    ")\n",
    "\n",
    "logging.info(\"# Generando semilla\\n\")\n",
    "np.random.seed(42)\n",
    "\n",
    "logging.info(\"# Cargar el conjunto de datos\\n\")\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "train_data = pd.read_csv('./load-data.csv')\n",
    "\n",
    "datos = train_data\n",
    "\n",
    "df = pd.DataFrame(datos)\n",
    "\n",
    "logging.info(\"#Mostrando datos originales:\\n\")\n",
    "logging.info(df)\n",
    "\n",
    "logging.info('\\nInformación de Columnas:\\n')\n",
    "logging.info(df.info())\n",
    "\n",
    "logging.info('# Creamos una copia para no modificar los datos originales')\n",
    "df_limpio = df.copy()\n",
    "\n",
    "logging.info('# 1. Manejo de valores faltantes')\n",
    "logging.info('# Rellenamos valores faltantes en columnas numéricas con la mediana')\n",
    "columnas_numericas = df_limpio.select_dtypes(include=['float64', 'int64']).columns\n",
    "for columna in columnas_numericas:\n",
    "    if (df_limpio[columna].isnull().sum() > 0) and (columna != 'HasCrCard'):\n",
    "        logging.info(f\"columna= {columna}\")\n",
    "        mediana = df_limpio[columna].median()\n",
    "        #df_limpio[columna].fillna(mediana, inplace=True)\n",
    "        df_limpio[columna] = df_limpio[columna].fillna(mediana)\n",
    "        logging.debug(f\"- Valores faltantes en '{columna}' rellenados con la mediana: {mediana}\")\n",
    "\n",
    "logging.info('# 2. Eliminación de duplicados')\n",
    "duplicados = df_limpio.duplicated().sum()\n",
    "df_limpio.drop_duplicates(inplace=True)\n",
    "logging.debug(f\"- Se eliminaron {duplicados} registros duplicados\")\n",
    "\n",
    "logging.info('# 3. Estandarización de texto')\n",
    "columnas_texto = df_limpio.select_dtypes(include=['object']).columns\n",
    "for columna in columnas_texto:\n",
    "    # Convertimos a minúsculas y eliminamos espacios externos\n",
    "    df_limpio[columna] = df_limpio[columna].str.lower().str.strip()\n",
    "    logging.debug(f\"- Columna '{columna}' estandarizada a minúsculas y sin espacios externos\")\n",
    "\n",
    "logging.info('# 4. Manejo de valores atípicos (outliers)')\n",
    "for columna in columnas_numericas:\n",
    "    if columna != 'HasCrCard':\n",
    "        Q1 = df_limpio[columna].quantile(0.25)\n",
    "        Q3 = df_limpio[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "        # Identificamos outliers\n",
    "        outliers = df_limpio[(df_limpio[columna] < limite_inferior) | \n",
    "                            (df_limpio[columna] > limite_superior)][columna]\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            # Recortamos los valores atípicos a los límites\n",
    "            df_limpio[columna] = df_limpio[columna].clip(limite_inferior, limite_superior)\n",
    "            logging.debug(f\"- Se encontraron y trataron {len(outliers)} valores atípicos en '{columna}'\")\n",
    "\n",
    "\n",
    "logging.info('# 6. Normalización de datos numéricos')\n",
    "for columna in columnas_numericas:\n",
    "    # Aplicamos normalización Min-Max\n",
    "    min_val = df_limpio[columna].min()\n",
    "    max_val = df_limpio[columna].max()\n",
    "    df_limpio[f'{columna}_normalizado'] = ((df_limpio[columna] - min_val) / \n",
    "                                          (max_val - min_val))\n",
    "    logging.debug(f\"- Columna '{columna}' normalizada entre 0 y 1\")\n",
    "\n",
    "logging.info('# 7. Conversión de tipos de datos')\n",
    "logging.info('# Preparando el formato de fecha')\n",
    "date_format = '%Y-%m-%d'\n",
    "\n",
    "logging.info('# Convertir columnas de fecha')\n",
    "for columna in columnas_texto:\n",
    "    try:\n",
    "        df_limpio[f'{columna}_fecha'] = pd.to_datetime(df_limpio[columna], format=date_format)\n",
    "        logging.info(f\"- Columna '{columna}' convertida a formato fecha\")\n",
    "    except:\n",
    "        logging.debug(f\"- La Columna '{columna}' no es de tipo fecha\")\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eae285-5dfa-4da5-9ff7-6e110d649aa1",
   "metadata": {},
   "source": [
    "### 2. Entrenamiento del modelo y valida con datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0aecd8-e40b-4cbc-affa-c5d38844044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de la variable objetivo (HasCrCard):\n",
      "HasCrCard\n",
      "1.0    82852\n",
      "0.0    27171\n",
      "Name: count, dtype: int64\n",
      "[0. 1.]\n",
      "HasCrCard\n",
      "1    82852\n",
      "0    27171\n",
      "Name: count, dtype: int64\n",
      "# Distribución después de SMOTE:\n",
      "\n",
      "HasCrCard\n",
      "0    57996\n",
      "1    57996\n",
      "Name: count, dtype: int64\n",
      "# XGBoost import XGBClassifier:\n",
      "\n",
      "# Hyperparametros:\n",
      "n_estimators=500: Número de árboles en el modelo.\n",
      "random_state=42: Fija la semilla para reproducibilidad.\n",
      "max_depth=3:  Profundidad máxima de cada árbol.\n",
      "eval_metric=logloss: Métrica de pérdida logarítmica usada internamente durante el entrenamiento.\n",
      "# Predicciones en el conjunto de prueba\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"# 8. Seleccionar columnas necesarias\\n\")\n",
    "final_dataset = df_limpio[['Geography', 'Gender', 'Age', 'NumOfProducts', 'EstimatedSalary','HasCrCard']]\n",
    "\n",
    "\n",
    "logging.info(\"# 9. Separar las características (X) y la variable objetivo (y)\")\n",
    "print(\"Distribución de la variable objetivo (HasCrCard):\")\n",
    "print(final_dataset['HasCrCard'].value_counts())\n",
    "print(final_dataset['HasCrCard'].unique())\n",
    "X = final_dataset.drop(columns=['HasCrCard'])\n",
    "y = final_dataset['HasCrCard'].apply(lambda x: 1 if x == 1. else 0) # Convertir 'si' a 1 y 'no' a 0 para SMOTE\n",
    "#y = final_dataset['HasCrCard']\n",
    "print(y.value_counts())\n",
    "\n",
    "\n",
    "logging.info(\"# 10. Aplicar One-Hot Encoding a las columnas categóricas\\n\")\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # Cambiamos sparse a sparse_output\n",
    "X_encoded = encoder.fit_transform(X[['Geography', 'Gender', 'Age']])\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(['Geography', 'Gender', 'Age']))\n",
    "\n",
    "logging.info(\"# 11. Combinar columnas codificadas con las numéricas\\n\")\n",
    "X_numeric = X[['NumOfProducts', 'EstimatedSalary']].reset_index(drop=True)\n",
    "X_final = pd.concat([X_numeric, X_encoded_df], axis=1)\n",
    "\n",
    "logging.info(\"# 12. Dividir datos en entrenamiento y prueba\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "logging.info(\"# 13. Aplicar SMOTE para balancear clases\\n\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"# Distribución después de SMOTE:\\n\")\n",
    "print(y_train_balanced.value_counts())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95177d9e-f1d4-414f-aa26-0baefea36816",
   "metadata": {},
   "source": [
    "### 3. Calcula y analiza las métricas de rendimiento (precisión, recall, F1-score, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b9fa4f-2c51-4132-aec0-7368298cc08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# XGBoost import XGBClassifier:\n",
      "\n",
      "# Hyperparametros:\n",
      "n_estimators=500: Número de árboles en el modelo.\n",
      "random_state=42: Fija la semilla para reproducibilidad.\n",
      "max_depth=3:  Profundidad máxima de cada árbol.\n",
      "eval_metric=logloss: Métrica de pérdida logarítmica usada internamente durante el entrenamiento.\n",
      "# Predicciones en el conjunto de prueba\n",
      "\n",
      "# Evaluación del modelo\n",
      "\n",
      "Accuracy del modelo: 0.752507043960372\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.00      0.00      8151\n",
      "           1       0.75      1.00      0.86     24856\n",
      "\n",
      "    accuracy                           0.75     33007\n",
      "   macro avg       0.52      0.50      0.43     33007\n",
      "weighted avg       0.64      0.75      0.65     33007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('# XGBoost import XGBClassifier:\\n')\n",
    "print('# Hyperparametros:')\n",
    "print('n_estimators=500: Número de árboles en el modelo.')\n",
    "print('random_state=42: Fija la semilla para reproducibilidad.')\n",
    "print('max_depth=3:  Profundidad máxima de cada árbol.')\n",
    "print('eval_metric=''logloss'': Métrica de pérdida logarítmica usada internamente durante el entrenamiento.')\n",
    "xgb_model = XGBClassifier(n_estimators=500, random_state=42, max_depth=3, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print('# Predicciones en el conjunto de prueba\\n')\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(f\"# Evaluación del modelo\\n\")\n",
    "print(f\"Accuracy del modelo: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"\\nClassification Report:\\n {classification_report(y_test, y_pred, zero_division=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0da55-c81f-4596-98c9-ba837b563c7c",
   "metadata": {},
   "source": [
    "### 4. Realiza ajustes de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa69b55-cfc6-4eb5-a9bb-175e02cfeb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Entrenar el modelo XGBoost Mejores Parametros\n",
      "# Ajustando Hyperparametros:\n",
      "----------------------------\n",
      "n_estimators=300: Número de árboles en el modelo.\n",
      "random_state=42: Fija la semilla para reproducibilidad.\n",
      "max_depth=12:  Profundidad máxima de cada árbol.\n",
      "eval_metric=logloss: Métrica de pérdida logarítmica usada internamente durante el entrenamiento.\n",
      "learning_rate=0.01: Asegura la tasa de aprendizaje\n",
      "colsample_bytree=0.8: Parametro de muestreo para delimitar el árbol.\n",
      "subsample=0.6: Fracción de muestra usada para cada árbol.\n",
      "Predicciones\n",
      "Accuracy del modelo: 0.753052382827885\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      8151\n",
      "           1       0.75      1.00      0.86     24856\n",
      "\n",
      "    accuracy                           0.75     33007\n",
      "   macro avg       0.88      0.50      0.43     33007\n",
      "weighted avg       0.81      0.75      0.65     33007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"# Entrenar el modelo XGBoost Mejores Parametros\")\n",
    "eval_set = [(X_train_balanced, y_train_balanced), (X_test, y_test)]\n",
    "\n",
    "print('# Ajustando Hyperparametros:')\n",
    "print('----------------------------')\n",
    "print('n_estimators=300: Número de árboles en el modelo.')\n",
    "print('random_state=42: Fija la semilla para reproducibilidad.')\n",
    "print('max_depth=12:  Profundidad máxima de cada árbol.')\n",
    "print('eval_metric=''logloss'': Métrica de pérdida logarítmica usada internamente durante el entrenamiento.')\n",
    "print('learning_rate=0.01: Asegura la tasa de aprendizaje')\n",
    "print('colsample_bytree=0.8: Parametro de muestreo para delimitar el árbol.')\n",
    "print('subsample=0.6: Fracción de muestra usada para cada árbol.')\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=300, random_state=42, max_depth=8, eval_metric='logloss', \n",
    "                          learning_rate=0.01, colsample_bytree=0.8, subsample=0.6)\n",
    "\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced, eval_set=eval_set, verbose=False)\n",
    "\n",
    "print(f\"Predicciones\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy del modelo: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"\\nClassification Report:\\n {classification_report(y_test, y_pred, zero_division=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f833245-a354-4e24-90c8-6747818362e2",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Nuestro modelo predecir un 75% de las veces si el cliente aceptará la tarjeta de crédito, sumado a una precisión de 75% y 100% en la clase 1 (recall) la cual es la que nos interesa precedir para que tome la tarjeta de crédito."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
